# -*- coding: utf-8 -*-
"""
Created on Fri Mar  8 15:04:43 2019

@author: v_fdwang
"""
import os
import random
import numpy as np
import torch as t
from torch.autograd import Variable as V
from torch import nn
from torch.nn import functional as F
from torch import optim



class Model(nn.Module):
    def __init__(self, conf):
        super(Model, self).__init__()
        self.conf = conf
        #self.use_cuda = self.conf["use_cuda"]
        self.emb = nn.Embedding(self.conf["dic_size"], self.conf["emb_size"])
        self.lin1 = nn.Linear(self.conf["emb_size"]*self.conf["max_push_len"]+self.conf["emb_size"], self.conf["lin1_size"])
        self.lin2 = nn.Linear(self.conf["lin1_size"], self.conf["out_size"])
        
    def forward(self, x):
        """
            input: 
                x: a Variable with size batch_size * N  N is equal to max_push_len
            output:
                y: a Variable with size batch_size * 1
        """
        batch_size, push_len = x.size()
        if push_len != self.conf["max_push_len"]:
            raise ValueError("You need padding first!")
        
        #if self.use_cuda:
        #    x = x.cuda()
        
        x_1 = self.emb(x) # batch_size * N * 300
        
        x_21 = t.sum(x_1, 1) # batch_size * 300
        
        x_22 = x_1.view(batch_size, -1)
        
        x_3 = t.cat([x_21, x_22], 1)
        
        x_4 = F.relu(self.lin1(x_3))
        
        y = self.lin2(x_4)
        
        return y
    
    @classmethod
    def test(cls, conf):
        temp = V(t.LongTensor([[1,2,3,2,1,0,0,0,0,0],[3,4,2,5,0,0,0,0,0,0]]))
        y = cls(conf)(temp)
        print("Done! Output: ", y)
        
    @staticmethod
    def critic(x, pre_model_path='F:/data/model.pth'):
        if not os.path.isfile(pre_model_path):
            raise ValueError("Check your model path and make sure you have a trained model!")
            return None
        pass
        


class Generator(nn.Module):
    """Generator """
    def __init__(self, conf):
        super(Generator, self).__init__()
        self.conf = conf
        self.num_emb = self.conf["num_emb"] # we have a up bound here, its cate num!
        self.emb_dim = self.conf["emb_size"]
        self.hidden_dim = self.conf["hidden_dim"]
        self.use_cuda = self.conf["use_cuda"]
        self.emb = nn.Embedding(self.num_emb, self.emb_dim)
        self.lstm = nn.LSTM(self.emb_dim, self.hidden_dim, batch_first=True)
        self.lin = nn.Linear(self.hidden_dim, self.num_emb)
        self.softmax = nn.LogSoftmax()
        self.init_params()

    def forward(self, x):
        """
        Args:
            x: (batch_size, seq_len), sequence of tokens generated by generator
        """
        emb = self.emb(x)
        h0, c0 = self.init_hidden(x.size(0))
        output, (h, c) = self.lstm(emb, (h0, c0))
        pred = self.softmax(self.lin(output.contiguous().view(-1, self.hidden_dim)))
        return pred

    def step(self, x, h, c):
        """
        Args:
            x: (batch_size,  1), sequence of tokens generated by generator
            h: (1, batch_size, hidden_dim), lstm hidden state
            c: (1, batch_size, hidden_dim), lstm cell state
        """
        emb = self.emb(x)
        output, (h, c) = self.lstm(emb, (h, c))
        pred = F.softmax(self.lin(output.view(-1, self.hidden_dim)), dim=1)
        return pred, h, c


    def init_hidden(self, batch_size):
        h = V(t.zeros((1, batch_size, self.hidden_dim)))
        c = V(t.zeros((1, batch_size, self.hidden_dim)))
        if self.use_cuda:
            h, c = h.cuda(), c.cuda()
        return h, c

    def init_params(self):
        for param in self.parameters():
            param.data.uniform_(-0.05, 0.05)

    def sample(self, batch_size, seq_len, x=None):
        #res = []
        flag = False # whether sample from zero
        if x is None:
            flag = True
        if flag:
            x = V(t.zeros((batch_size, 1)).long())
        if self.use_cuda:
            x = x.cuda()
        h, c = self.init_hidden(batch_size)
        samples = []
        if flag:
            for i in range(seq_len):
                output, h, c = self.step(x, h, c)
                x = output.multinomial(1)
                samples.append(x)
        else:
            given_len = x.size(1)
            lis = x.chunk(x.size(1), dim=1)
            for i in range(given_len):
                output, h, c = self.step(lis[i], h, c)
                samples.append(lis[i])
            x = output.multinomial(1)
            for i in range(given_len, seq_len):
                samples.append(x)
                output, h, c = self.step(x, h, c)
                x = output.multinomial(1)
        output = t.cat(samples, dim=1)
        return output    
        
        